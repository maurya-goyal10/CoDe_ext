import json
import copy
import torch
import torchvision

from pathlib import Path
from tqdm.auto import tqdm
from diffusers import StableDiffusionPipeline
from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput
from typing import Union, Optional, List, Callable, Dict, Any, Tuple
from scorers import HPSScorer, AestheticScorer, FaceRecognitionScorer, ClipScorer,ImageRewardScorer, PickScoreScorer, MultiReward, CompressibilityScorer
from diffusers.schedulers.scheduling_ddpm import DDPMSchedulerOutput, DDPMScheduler

from sklearn.cluster import DBSCAN, KMeans
from itertools import combinations
import numpy as np

from sampler import sampling
from cluster import clustering
from guidance_scaling import scaling_guidance

from torchopt.diff.zero_order import zero_order
from torch.distributions import Normal

class CoDeGradSDFinalI2IGeneral(StableDiffusionPipeline):
    @torch.no_grad()
    def __call__(
        self,
        offset: int = 5,
        prompt: Union[str, List[str]] = None,
        height: Optional[int] = None,
        percent_noise: float = 1.0,
        width: Optional[int] = None,
        num_inference_steps: int = 50,
        num_try: Optional[int] = 1,
        guidance_scale: float = 7.5,
        n_samples: int = 5,
        block_size: int = 5,
        negative_prompt: Optional[Union[str, List[str]]] = None,
        num_images_per_prompt: Optional[int] = 1,
        eta: float = 0.0,
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        latents: Optional[torch.FloatTensor] = None,
        prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_prompt_embeds: Optional[torch.FloatTensor] = None,
        output_type: Optional[str] = "pil",
        return_dict: bool = True,
        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,
        callback_steps: int = 1,
        cross_attention_kwargs: Optional[Dict[str, Any]] = None,
    ):
        r"""
        Function invoked when calling the pipeline for generation.
        Args:
            prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.
                instead.
            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):
                The height in pixels of the generated image.
            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):
                The width in pixels of the generated image.
            num_inference_steps (`int`, *optional*, defaults to 50):
                The number of denoising steps. More denoising steps usually lead to a higher quality image at the
                expense of slower inference.
            guidance_scale (`float`, *optional*, defaults to 7.5):
                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
                `guidance_scale` is defined as `w` of equation 2. of [Imagen
                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >
                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,
                usually at the expense of lower image quality.
            negative_prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation. If not defined, one has to pass
                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
                less than `1`).
            num_images_per_prompt (`int`, *optional*, defaults to 1):
                The number of images to generate per prompt.
            eta (`float`, *optional*, defaults to 0.0):
                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to
                [`schedulers.DDIMScheduler`], will be ignored for others.
            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):
                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
                to make generation deterministic.
            latents (`torch.FloatTensor`, *optional*):
                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image
                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
                tensor will ge generated by sampling using the supplied random `generator`.
            prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not
                provided, text embeddings will be generated from `prompt` input argument.
            negative_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input
                argument.
            output_type (`str`, *optional*, defaults to `"pil"`):
                The output format of the generate image. Choose between
                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a
                plain tuple.
            callback (`Callable`, *optional*):
                A function that will be called every `callback_steps` steps during inference. The function will be
                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.
            callback_steps (`int`, *optional*, defaults to 1):
                The frequency at which the `callback` function will be called. If not specified, the callback will be
                called at every step.
            cross_attention_kwargs (`dict`, *optional*):
                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under
                `self.processor` in
                [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).
        Examples:
        Returns:
            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:
            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.
            When returning a tuple, the first element is a list with the generated images, and the second element is a
            list of `bool`s denoting whether the corresponding generated image likely represents "not-safe-for-work"
            (nsfw) content, according to the `safety_checker`.
        """
        prompt_embeds_ = prompt_embeds
        # 0. Default height and width to unet
        height = height or self.unet.config.sample_size * self.vae_scale_factor
        width = width or self.unet.config.sample_size * self.vae_scale_factor

        # 1. Check inputs. Raise error if not correct
        self.check_inputs(
            prompt, height, width, callback_steps, negative_prompt, prompt_embeds, negative_prompt_embeds
        )

        # 2. Define call parameters
        if prompt is not None and isinstance(prompt, str):
            batch_size = 1
        elif prompt is not None and isinstance(prompt, list):
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]

        device = self._execution_device

        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
        # corresponds to doing no classifier free guidance.
        do_classifier_free_guidance = guidance_scale > 1.0

        # Generate in batches
        assert n_samples % self.genbatch == 0

        # 3. Encode input prompt
        prompt_embeds = self._encode_prompt(
            prompt,
            device,
            n_samples * self.genbatch,
            # num_images_per_prompt,
            do_classifier_free_guidance,
            negative_prompt,
            prompt_embeds=prompt_embeds,
            negative_prompt_embeds=negative_prompt_embeds,
        )
        
        # 4. Prepare timesteps
        self.scheduler.set_timesteps(num_inference_steps, device=device)
        timesteps = self.scheduler.timesteps

        # 5. Prepare latent variables
        num_channels_latents = self.unet.config.in_channels
        latents = self.prepare_latents(
            batch_size * num_images_per_prompt,
            num_channels_latents,
            height,
            width,
            prompt_embeds.dtype,
            device,
            generator,
            latents,
        )

        # 6. Prepare extra step kwargs.
        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)

        # 7. Denoising loop for BoN 
        is_failed = True
        num_batch = num_images_per_prompt // self.genbatch
        
        for batch_iter in tqdm(range(num_batch), total=num_batch):
            
            if getattr(self, 'samples_schedule', None) is not None:
                self.val_sample_size = len(timesteps)/len(self.samples_schedule)
                self.next_sample_size = self.val_sample_size
                self.sample_idx = 0
                n_samples = self.samples_schedule[0]
                prompt_embeds = self._encode_prompt(
                    prompt,
                    device,
                    n_samples * self.genbatch,
                    # num_images_per_prompt,
                    do_classifier_free_guidance,
                    negative_prompt,
                    prompt_embeds=prompt_embeds_,
                    negative_prompt_embeds=negative_prompt_embeds,
                )

            curr_samples = latents[batch_iter * self.genbatch: (batch_iter + 1) * self.genbatch]
            curr_samples = curr_samples.repeat(n_samples, 1, 1, 1) # (n_samples, 4, 64, 64)
            
            num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order
            step_counter = 0
            for i, t in enumerate(timesteps):
                
                if t > 1000 * percent_noise:
                    continue

                step_counter += 1
                
                if getattr(self, 'samples_schedule', None) is not None:
                    # print(f"At time {t} the shape of curr samples is {curr_samples.shape}")
                    if(i>=self.next_sample_size):
                        # print(f"At {t} rewards are {rewards} {rewards.argmax()}")
                        # batch_max_reward = torch.max(rewards)
                        # print(f"At timestep {t} max rewards out of {rewards} is {batch_max_reward}")
                        # batch_prev_reward.append(batch_max_reward)
                        self.sample_idx += 1
                        self.next_sample_size += self.val_sample_size
                        if n_samples != self.samples_schedule[self.sample_idx]:
                            n_samples = self.samples_schedule[self.sample_idx]
                            if len(curr_samples) >= n_samples:
                                curr_samples = curr_samples[:n_samples]
                            else:
                                repeats = int(np.ceil(n_samples / len(curr_samples)))
                                curr_samples = (curr_samples.repeat(repeats, 1, 1, 1))[:n_samples]
                            # curr_samples = curr_samples[:n_samples]
                            prompt_embeds = self._encode_prompt(
                                prompt,
                                device,
                                n_samples * self.genbatch,
                                # num_images_per_prompt,
                                do_classifier_free_guidance,
                                negative_prompt,
                                prompt_embeds=prompt_embeds_,
                                negative_prompt_embeds=negative_prompt_embeds,
                            )
                            
                # print(f"Timestep {t} the shape of curr_samples is {curr_samples.shape}")
                # expand the latents if we are doing classifier free guidance
                # latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents
                
                # print(f"t:{t.item()}")
                latent_model_input = torch.cat([curr_samples] * 2) if do_classifier_free_guidance else curr_samples
                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)

                # predict the noise residual
                noise_pred = self.unet(
                    latent_model_input,
                    t,
                    encoder_hidden_states=prompt_embeds,
                    cross_attention_kwargs=cross_attention_kwargs,
                ).sample

                # perform guidance
                if do_classifier_free_guidance:
                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                    correction = noise_pred_text - noise_pred_uncond
                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)
                    
                # print(f"Norm of the correction before is {correction.norm().item()}")

                # compute the previous noisy sample x_t -> x_t-1
                # latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample
                curr_samples = self.scheduler.step(noise_pred, t, curr_samples, **extra_step_kwargs).prev_sample

                prev_timestep = t - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps
                
                if ((i + 1) % self.grad_blocksize == 0) and t<=self.start_time*1000 and t>= self.end_time*1000 and self.target_guidance > 0.0: 
                    # print(t.item())
                    if t > timesteps[-1]: # If not final step use estimates x0    
                        # Gradient Guidance with clustering   
                        if self.do_clustering:
                            # print(f"{t.item()} {self.clustering_method}")
                            # print(curr_samples.shape)
                            curr_samples_np = curr_samples.reshape(n_samples,-1).cpu().numpy()
                            # print(curr_samples_np.shape)
                            
                            n_clusters,labels = clustering.cluster(curr_samples_np,self.clustering_method)
                            cluster_means = torch.stack([curr_samples[labels == i].mean(dim=0) for i in range(n_clusters)])
                            # print(f"The shape of the cluster means is: {cluster_means.shape}")

                            
                            # new_samples = []
                            latent_model_input = torch.cat([cluster_means] * 2) if do_classifier_free_guidance else cluster_means
                            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)

                            # predict the noise residual
                            noise_pred = self.unet(
                                latent_model_input,
                                prev_timestep,
                                encoder_hidden_states=torch.cat([prompt_embeds[:n_clusters],prompt_embeds[n_samples:n_samples+n_clusters]],dim=0),
                                cross_attention_kwargs=cross_attention_kwargs,
                            ).sample

                            # perform guidance
                            if do_classifier_free_guidance:
                                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                                correction = noise_pred_text - noise_pred_uncond
                                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)
                                
                            # print(f"Norm of the correction after is {correction.norm().item()}")
                                
                            # print(f"The labels are {labels}")
                            for label in range(n_clusters):
                                indices = np.where(labels == label)[0]
                                    
                                grad = self.compute_gradient(cluster_means[[label]], prompt, noise_pred[[label]], prev_timestep)
                                target_guidance = scaling_guidance.set_scale(grad,correction[[label]],self.target_guidance,guidance_scale,self.guidance_method)
                                # grad_norm = (grad * grad).mean().sqrt().item()
                                # # print(f"The norm of the grad is {grad_norm}")
                                # target_guidance = (correction[[label]] * correction[[label]]).mean().sqrt().item() * guidance_scale / (grad_norm + 1e-8) * self.target_guidance 
                                # # print(f"guidance scale is: {target_guidance}") 
                                # if target_guidance > 150.0: target_guidance = 150.0
                                # print(f"t: {t.item()} the target_guidance is {target_guidance} the grad norm is {grad_norm}")
                                # print(f"Cluster points shape is {cluster_points.shape} and the grad shape is {grad.shape}")
                                curr_samples[indices] = curr_samples[indices] + target_guidance*grad
                                
                            # curr_samples = torch.cat(new_samples,dim=0)
                            # print(curr_samples.shape)
                            
                        else:
                            latent_model_input = torch.cat([curr_samples] * 2) if do_classifier_free_guidance else curr_samples
                            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)

                            # predict the noise residual
                            noise_pred = self.unet(
                                latent_model_input,
                                prev_timestep,
                                encoder_hidden_states=prompt_embeds,
                                cross_attention_kwargs=cross_attention_kwargs,
                            ).sample

                            # perform guidance
                            if do_classifier_free_guidance:
                                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                                correction = (noise_pred_text - noise_pred_uncond)
                                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)
                            
                            # batch_size_grad = 1
                            # if hasattr(self,'batch_size_grad'):
                            #     batch_size_grad  = self.batch_size_grad
                                
                            for k in range(curr_samples.shape[0]):
                                grad_k = self.compute_gradient(curr_samples[[k]], prompt, noise_pred[[k]], prev_timestep)
                                # grad_k_2 = self.compute_gradient(curr_samples[[k]], prompt, noise_pred[[k]], prev_timestep)
                                # print(f"{t.item()} {torch.allclose(grad_k, grad_k_2, atol=1e-6)}")
                                # grad_k = self.compute_gradient(curr_samples[k:k+batch_gradient].unsqueeze(0), prompt, noise_pred[k:k+batch_gradient], prev_timestep)
                                # target_guidance = scaling_guidance.set_scale(grad_k,correction[k:k+(batch_size_grad)],self.target_guidance,guidance_scale,self.guidance_method)
                                target_guidance = scaling_guidance.set_scale(grad_k,correction[[k]],self.target_guidance,guidance_scale,self.guidance_method)
                                # grad_norm = (grad_k * grad_k).mean().sqrt().item()
                                # target_guidance = (correction * correction).mean().sqrt().item() * guidance_scale / (grad_norm + 1e-8) * self.target_guidance 
                                # if target_guidance > 150.0: target_guidance = 150.0
                                # print(f"t: {t.item()}, k: {k}, the target_guidance is {target_guidance} the grad norm is {grad_norm}")
                                # curr_samples[k:k+(batch_size_grad)] = curr_samples[k:k+(batch_size_grad)] + target_guidance[:,None,None,None]*grad_k
                                curr_samples[[k]] = curr_samples[[k]] + target_guidance*grad_k
                                # print(f"k: {k} The shape of the curr samples is {curr_samples.shape} and the shape of the grad is {grad_k.shape} ")

                if ((i + 1) % block_size == 0 or t==timesteps[-1]):
                    if t>timesteps[-1]:
                        # Get the noise prediction for after taking the gradient step
                        latent_model_input = torch.cat([curr_samples] * 2) if do_classifier_free_guidance else curr_samples
                        latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)

                        # predict the noise residual
                        noise_pred = self.unet(
                            latent_model_input,
                            prev_timestep,
                            encoder_hidden_states=prompt_embeds,
                            cross_attention_kwargs=cross_attention_kwargs,
                        ).sample

                        # perform guidance
                        if do_classifier_free_guidance:
                            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)
                            
                        pred_original_temp = self.scheduler.step(noise_pred, prev_timestep, curr_samples, **extra_step_kwargs).pred_original_sample
                        rewards = self.compute_scores(pred_original_temp, prompt)
                    else:
                        if isinstance(self.scorer, MultiReward):
                            rewards,rewards1,rewards2 = self.compute_scores(curr_samples, prompt,return_all=True)
                        elif isinstance(self.scorer, PickScoreScorer):
                            rewards_igram = self.compute_score_igram(curr_samples)
                        else:
                            rewards = self.compute_scores(curr_samples, prompt)
                        rewards = self.compute_scores(curr_samples, prompt)
                    
                    rewards = torch.cat([x.unsqueeze(0) for x in rewards.chunk(n_samples)], dim=0) # (n_samples, self.genbatch)
                    
                    gen_sample = copy.deepcopy(curr_samples)
                    gen_sample = torch.cat([x.unsqueeze(0) for x in gen_sample.chunk(n_samples)], dim=0) # (n_samples, self.genbatch, 4, 64, 64)
                    gen_sample = gen_sample.permute(1,0,2,3,4)
                    
                    # At the end just pick the best
                    if t<=timesteps[-1]:
                        select_ind = torch.max(rewards, dim=0)[1]
                        curr_samples = torch.cat([x[select_ind[idx]].unsqueeze(0) for idx, x in enumerate(gen_sample)], dim=0)
                    
                    # sampling techniques        
                    else:
                        curr_samples = sampling.sample(gen_sample,rewards,n_samples,temperature=getattr(self, 'temp', None),method=getattr(self, 'sampling', None))

            try:
                self.save_outputs(curr_samples, start=(batch_iter*self.genbatch)+offset, prompt=prompt, num_try=num_try)
            except:
                is_failed = True
                continue
            
            store_rewards = []
            savepath = Path(self.path.joinpath(prompt)).joinpath("rewards.json")
            if Path.exists(savepath): # if exists append else create new
                with open(savepath, 'r') as fp:
                    store_rewards = json.load(fp)

            rewards = rewards.permute(1,0)
            rewards = torch.cat([x[select_ind[idx]].unsqueeze(0) for idx, x in enumerate(rewards)], dim=0) # TODO: Make it efficient
            store_rewards.extend(rewards.cpu().numpy().tolist())

            with open(savepath, 'w') as fp:
                json.dump(store_rewards, fp)
                
            if isinstance(self.scorer, PickScoreScorer):
                store_igram_rewards = []
                rewards_igram = torch.cat([x.unsqueeze(0) for x in rewards_igram.chunk(n_samples)], dim=0)
                savepath_igram = Path(self.path.joinpath(prompt)).joinpath("rewards_igram.json")
                if Path.exists(savepath_igram): # if exists append else create new
                    with open(savepath_igram, 'r') as fp:
                        store_igram_rewards = json.load(fp)
                        
                rewards_igram = rewards_igram.permute(1,0)
                rewards_igram = torch.cat([x[select_ind[idx]].unsqueeze(0) for idx, x in enumerate(rewards_igram)], dim=0) # TODO: Make it efficient
                store_igram_rewards.extend(rewards_igram.cpu().numpy().tolist())

                with open(savepath_igram, 'w') as fp:
                    json.dump(store_igram_rewards, fp)
                
            if isinstance(self.scorer, MultiReward):
                store_rewards1 = []
                rewards1 = torch.cat([x.unsqueeze(0) for x in rewards1.chunk(n_samples)], dim=0) # (n_samples, self.genbatch)
                savepath1 = Path(self.path.joinpath(prompt)).joinpath("rewards1.json")
                if Path.exists(savepath1): # if exists append else create new
                    with open(savepath1, 'r') as fp:
                        store_rewards1 = json.load(fp)
                        

                rewards1 = rewards1.permute(1,0)
                rewards1 = torch.cat([x[select_ind[idx]].unsqueeze(0) for idx, x in enumerate(rewards1)], dim=0) # TODO: Make it efficient
                store_rewards1.extend(rewards1.cpu().numpy().tolist())

                with open(savepath1, 'w') as fp:
                    json.dump(store_rewards1, fp)
                    
                store_rewards2 = []
                rewards2 = torch.cat([x.unsqueeze(0) for x in rewards2.chunk(n_samples)], dim=0)
                savepath2 = Path(self.path.joinpath(prompt)).joinpath("rewards2.json")
                if Path.exists(savepath2): # if exists append else create new
                    with open(savepath2, 'r') as fp:
                        store_rewards2 = json.load(fp)
                        

                rewards2 = rewards2.permute(1,0)
                rewards2 = torch.cat([x[select_ind[idx]].unsqueeze(0) for idx, x in enumerate(rewards2)], dim=0) # TODO: Make it efficient
                store_rewards2.extend(rewards2.cpu().numpy().tolist())

                with open(savepath2, 'w') as fp:
                    json.dump(store_rewards2, fp)

        return is_failed
    
    def get_variance(self, t, predicted_variance=None, variance_type=None):
        
        prev_t = self.scheduler.previous_timestep(t)

        alpha_prod_t = self.scheduler.alphas_cumprod[t]
        alpha_prod_t_prev = self.scheduler.alphas_cumprod[prev_t] if prev_t >= 0 else self.scheduler.one
        current_beta_t = 1 - alpha_prod_t / alpha_prod_t_prev

        # For t > 0, compute predicted variance βt (see formula (6) and (7) from https://arxiv.org/pdf/2006.11239.pdf)
        # and sample from it to get previous sample
        # x_{t-1} ~ N(pred_prev_sample, variance) == add variance to pred_sample
        variance = (1 - alpha_prod_t_prev) / (1 - alpha_prod_t) * current_beta_t

        # we always take the log of variance, so clamp it to ensure it's not 0
        variance = torch.clamp(variance, min=1e-20)

        if variance_type is None:
            variance_type = self.scheduler.config.variance_type

        # hacks - were probably added for training stability
        if variance_type == "fixed_small":
            variance = variance
        # for rl-diffuser https://arxiv.org/abs/2205.09991
        elif variance_type == "fixed_small_log":
            variance = torch.log(variance)
            variance = torch.exp(0.5 * variance)
        elif variance_type == "fixed_large":
            variance = current_beta_t
        elif variance_type == "fixed_large_log":
            # Glide max_log
            variance = torch.log(current_beta_t)
        elif variance_type == "learned":
            return predicted_variance
        elif variance_type == "learned_range":
            min_log = torch.log(variance)
            max_log = torch.log(current_beta_t)
            frac = (predicted_variance + 1) / 2
            variance = frac * max_log + (1 - frac) * min_log

        return variance
    
    def set_grad_blocksize(self, grad_blocksize=5):
        self.grad_blocksize = grad_blocksize

    def set_guidance(self, target_guidance: int = 150):
        self.target_guidance = target_guidance
        
    def set_start_time(self, start_time: float=1.0):
        self.start_time = start_time
        
    def set_end_time(self, end_time: float=0.0):
        self.end_time = end_time
    
    def set_genbatch(self, genbatch: int = 5):
        self.genbatch = genbatch
        
    def set_temp(self,temp: int=200):
        self.temp = temp

    def set_retry(self, retry: int = 0):
        self.retry = retry
    
    def set_project_path(self, path):
        self.path = path

    def setup_scorer(self, scorer):
        self.scorer = scorer

    def set_do_clustering(self, do_clustering):
        self.do_clustering = do_clustering
        
    def set_clustering_method(self, clustering_method):
        self.clustering_method = clustering_method
        
    def set_sampling(self, sampling):
        self.sampling = sampling
        
    def set_guidance_method(self,guidance_method):
        self.guidance_method = guidance_method
        
    def set_sampling_method(self,clustering_method):
        self.clustering_method = clustering_method
        
    def set_samples_schedule(self,samples_schedule):
        self.samples_schedule = samples_schedule
        
    def set_zoo_method(self,zoo_method):
        self.zoo_method = zoo_method
        
    def set_zoo_n_sample(self,zoo_n_sample):
        self.zoo_n_sample = zoo_n_sample
        
    def set_batch_size_grad(self,batch_size_grad):
        self.batch_size_grad = batch_size_grad
        
    def set_clipscorer(self):
        self.clipscorer = ClipScorer()
        
    def save_outputs(self, latent, prompt, start, num_try):
        decoded_latents = self.decode_latents(latent)
        image_pils = self.numpy_to_pil(decoded_latents)

        decoded_latents = torch.from_numpy(decoded_latents).permute(0,3,1,2)

        try:
            if isinstance(self.scorer, HPSScorer)or\
                isinstance(self.scorer, ImageRewardScorer) or\
                isinstance(self.scorer, PickScoreScorer) or\
                isinstance(self.scorer, MultiReward):
                prompts = [prompt] * len(decoded_latents)
                rewards = self.scorer.score(decoded_latents, prompts)
            elif isinstance(self.scorer, FaceRecognitionScorer)or\
                isinstance(self.scorer, ClipScorer):
                rewards = self.scorer.score(decoded_latents, self.target_img)
            else:
                rewards = self.scorer.score(decoded_latents)
        except:
            if num_try < self.retry:
                raise ValueError()
            else:
                rewards = torch.tensor([- torch.inf] * decoded_latents.shape[0])

        savepath = Path(self.path.joinpath(prompt))
        if not Path.exists(savepath):
            Path.mkdir(savepath, exist_ok=True, parents=True)

        for idx in range(len(image_pils)):
            image_pils[idx].save(savepath.joinpath(f"{start + idx}.png"))

        return rewards.detach().cpu().tolist()
    
    def set_target(self, target_img):
        if isinstance(self.scorer, ClipScorer):
            target_img = torchvision.transforms.ToTensor()(target_img)
            self.target_img = self.scorer.encode(target_img.unsqueeze(0))
        elif isinstance(self.scorer, PickScoreScorer):
            target_img = torchvision.transforms.ToTensor()(target_img)
            self.target_img = self.clipscorer.encode(target_img.unsqueeze(0))
        else:
            self.target_img = torchvision.transforms.ToTensor()(target_img)
        # print(self.target_img.shape)

    @torch.no_grad()
    def compute_scores(self, latent, prompt,return_all=False):
        decoded_latents = self.decode_latents(latent)
        decoded_latents = torch.from_numpy(decoded_latents).permute(0,3,1,2)
        
        if return_all and isinstance(self.scorer, MultiReward):
            prompts = [prompt] * len(decoded_latents)
            out = self.scorer.score(decoded_latents, prompts,return_all=True)
            return out

        if isinstance(self.scorer, HPSScorer) or\
            isinstance(self.scorer, ImageRewardScorer) or\
            isinstance(self.scorer, PickScoreScorer) or\
            isinstance(self.scorer, MultiReward):
            
            prompts = [prompt] * len(decoded_latents)
            out = self.scorer.score(decoded_latents, prompts)

        elif isinstance(self.scorer, FaceRecognitionScorer) or\
              isinstance(self.scorer, ClipScorer):
            
            out = self.scorer.score(decoded_latents, self.target_img)
        else:
            out = self.scorer.score(decoded_latents)

        return out
    
    @torch.no_grad()
    def compute_score_igram(self,latent):
        decoded_latents = self.decode_latents(latent)
        decoded_latents = torch.from_numpy(decoded_latents).permute(0,3,1,2)
        
        return self.clipscorer.score(decoded_latents, self.target_img)
    
    @torch.enable_grad()
    def compute_gradient(self, latents, prompt, noise_pred, t):
        # print(f"The shape of the latents is {latents.shape} ")
        # Enable gradient computation for latents
        latent_in = latents.detach().requires_grad_(True)
        
        # Get predictions for the full batch
        # with torch.cuda.amp.autocast(dtype=torch.float16):
        pred_original_sample = predict_x0_from_xt(
            self.scheduler,
            noise_pred,
            t,
            latent_in
        )
        
        # print(f"The shape of the predicted original sample is: {pred_original_sample.shape}")
        
        self.vae.eval()
        # self.scorer.eval()
        
        im_pix_un = self.vae.decode(
            pred_original_sample.to(self.vae.dtype) / self.vae.config.scaling_factor,
            return_dict = False
        )[0]
        # im_pix_un = self.vae.decode(pred_original_sample.to(self.vae.dtype) / self.vae.config.scaling_factor).sample
            
        # Process images
        im_pix = ((im_pix_un / 2) + 0.5).clamp(0, 1).cpu()
        # print(f"The shape of the image is: {im_pix.shape}")
        # print(im_pix.shape)
        
        # Compute rewards/loss based on scorer type
        if isinstance(self.scorer, (HPSScorer,PickScoreScorer,MultiReward)):
            prompts = [prompt] * len(im_pix)
            rewards = -1 * self.scorer.loss_fn(im_pix, prompts)
        elif isinstance(self.scorer, (FaceRecognitionScorer, ClipScorer)):
            rewards = -1 * self.scorer.loss_fn(im_pix, self.target_img)
        else:
            rewards = -1 * self.scorer.loss_fn(im_pix)
            
        if isinstance(self.scorer, (CompressibilityScorer)):
            def loss_fn(latents):
                # Recompute im_pix within the function scope
                pred_original_sample = predict_x0_from_xt(
                    self.scheduler, noise_pred, t, latents
                )
                im_pix_un = self.vae.decode(pred_original_sample.to(self.vae.dtype) / self.vae.config.scaling_factor, return_dict=False)[0]
                im_pix = ((im_pix_un / 2) + 0.5).clamp(0, 1).cpu()

                rewards = -1 * self.scorer.loss_fn(im_pix)

                return rewards.sum()
            
            # grad_fn = zero_order(distribution=Normal(torch.zeros_like(latent_in),torch.ones_like(latent_in)))(loss_fn)
            grad_fn = zero_order(distribution=Normal(torch.zeros_like(latent_in[0,0,0,0]),torch.ones_like(latent_in[0,0,0,0])),method=self.zoo_method,num_samples=self.zoo_n_sample)(loss_fn)
            loss = grad_fn(latent_in)
            grad = torch.autograd.grad(loss.sum(), latent_in)[0].detach()
            
            # print(grad.shape)
            return grad
        # Compute gradient
        grad = torch.autograd.grad(rewards.sum(), latent_in)[0].detach()
        # grad = torch.autograd.grad(rewards, latent_in,grad_outputs=torch.ones_like(rewards))[0].detach()
        
        # Clean up to free memory
        del im_pix_un, im_pix, rewards
        torch.cuda.empty_cache()
        
        return grad.cuda().clone()
    
def predict_x0_from_xt(
    self: DDPMScheduler,
    model_output: torch.FloatTensor,
    timestep: int,
    sample: torch.FloatTensor,
) -> Union[DDPMSchedulerOutput, Tuple]:
    
    assert isinstance(self, DDPMScheduler)
    if self.num_inference_steps is None:
        raise ValueError(
            "Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler"
        )

    t = timestep

    prev_t = self.previous_timestep(t)

    if model_output.shape[1] == sample.shape[1] * 2 and self.variance_type in ["learned", "learned_range"]:
        model_output, predicted_variance = torch.split(model_output, sample.shape[1], dim=1)
    else:
        predicted_variance = None
        
    # print(model_output.shape)

    # 1. compute alphas, betas
    alpha_prod_t = self.alphas_cumprod[t]
    alpha_prod_t_prev = self.alphas_cumprod[prev_t] if prev_t >= 0 else self.one
    beta_prod_t = 1 - alpha_prod_t
    current_alpha_t = alpha_prod_t / alpha_prod_t_prev

    # 2. compute predicted original sample from predicted noise also called
    # "predicted x_0" of formula (15) from https://arxiv.org/pdf/2006.11239.pdf
    if self.config.prediction_type == "epsilon":
        pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)
    elif self.config.prediction_type == "sample":
        pred_original_sample = model_output
    elif self.config.prediction_type == "v_prediction":
        pred_original_sample = (alpha_prod_t**0.5) * sample - (beta_prod_t**0.5) * model_output
    else:
        raise ValueError(
            f"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample` or"
            " `v_prediction`  for the DDPMScheduler."
        )

    # 3. Clip or threshold "predicted x_0"
    if self.config.thresholding:
        pred_original_sample = self._threshold_sample(pred_original_sample)
    elif self.config.clip_sample:
        pred_original_sample = pred_original_sample.clamp(
            -self.config.clip_sample_range, self.config.clip_sample_range
        )

    return pred_original_sample.to(dtype=sample.dtype)
